{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training\n",
    "x = [0, 1, 2, 3, 4]\n",
    "y = [0.1, 0.9, 2.1, 2.9, 4.1]\n",
    "\n",
    "# predicted\n",
    "y_1 = [0, 1, 2, 3, 4]\n",
    "y_2 = [0.1, 0.9, 2.1, 2.9, 4.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y: list, y_hat: list) -> float:\n",
    "    \"\"\"\n",
    "    Calculates and returns the RMSE of the given lists.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y : list\n",
    "        A list of numeric values.\n",
    "    y_hat : list\n",
    "        A list of numeric values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        The Root Mean Square Error of the two given lists.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AssertionError\n",
    "        * If y or y_hat are not of type list.\n",
    "        * If y and y_hat are not the same length.\n",
    "    \"\"\"\n",
    "    \n",
    "    # perform some simple, but not exhaustive assertions on the input parameters\n",
    "    assert type(y) is list, f'y must be of type list, but found {type(y)}'\n",
    "    assert type(y_hat) is list, f'y_hat must be of type list, but found {type(y_hat)}'\n",
    "    assert len(y) == len(y_hat), f'y and y_hat must be the same length, but are {len(y)} and {len(y_hat)}, respectively.' \n",
    "    \n",
    "    return np.sqrt(np.square(np.array(y) - np.array(y_hat)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for model 1: 0.09999999999999996\n",
      "RMSE for model 2: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"RMSE for model 1: {rmse(y, y_1)}\")\n",
    "print(f\"RMSE for model 2: {rmse(y, y_2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pseudo-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rmse(y, [1])\n",
    "    raise Exception('No assertion error!')\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rmse(y, None)\n",
    "    raise Exception('No assertion error!')\n",
    "except AssertionError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    rmse([1,2], ['a', 'b'])\n",
    "    raise Exception('No type error!')\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Positive Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rmse([1, 2], [2, 3]) == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert rmse(y, y) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "\n",
    "#### Which algorithm is better?\n",
    "\n",
    "Based on the small amount of data given for both training and testing the models, it is difficult to determine which model performed better. Did *Model 1* overfit? Would cross-validation, in the face of a lack of test data, help predict which model might generalize?\n",
    "\n",
    "That said, given the data in this example, it is my belief that the two models performed similarly, if not equally well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
